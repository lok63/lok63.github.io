<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>leonidas.ai</title><description>Welcome to my AI blog</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>leonidas.ai</title><link>http://localhost:2368/</link></image><generator>Ghost 3.0</generator><lastBuildDate>Tue, 19 Nov 2019 15:38:21 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>BM25</title><description>&lt;p&gt;Understanding BM25 requires a lot of patience at first, but later on, trust me you will be able to use it like a pro. When I started reading about this ranking function my head was aching but after some time things got simpler.&lt;/p&gt;&lt;p&gt;In short BM25 is:&lt;/p&gt;&lt;h2 id="1-okabi-bm25"&gt;1. Okabi BM25&lt;/h2&gt;</description><link>http://localhost:2368/bm25/</link><guid isPermaLink="false">5dd16832eaedf3bb6610df8a</guid><dc:creator>Leonidas Cosnatntinou</dc:creator><pubDate>Sun, 17 Nov 2019 15:33:12 GMT</pubDate><media:content url="http://localhost:2368/content/images/2019/11/bm25_cover.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2019/11/bm25_cover.jpg" alt="BM25"&gt;&lt;p&gt;Understanding BM25 requires a lot of patience at first, but later on, trust me you will be able to use it like a pro. When I started reading about this ranking function my head was aching but after some time things got simpler.&lt;/p&gt;&lt;p&gt;In short BM25 is:&lt;/p&gt;&lt;h2 id="1-okabi-bm25"&gt;1. Okabi BM25 &lt;/h2&gt;&lt;ul&gt;&lt;li&gt;BM is an abbreviation of best matching&lt;/li&gt;&lt;li&gt;scoring function&lt;/li&gt;&lt;li&gt;Information Retrieval&lt;/li&gt;&lt;li&gt;Ranking function used for search engines&lt;/li&gt;&lt;li&gt;Probabilistic Method ????&lt;/li&gt;&lt;li&gt;given a Query —&amp;gt;  find the most relevant documents&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2 id="2-okapi-bm25"&gt;2.Okapi BM25&lt;/h2&gt;&lt;p&gt;Okapi BM25 is a scoring function used in information retrieval to find the most relevant documents given a query. It is considered a probabilistic method and it’s widely used as a ranking function for search engines. Because BM25 is a customised version of TF-IDF for scoring document, it’s important to first understand this first and then dive into the BM25 ranking algorithm. Here is the ranking function:&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2019/11/bm25.png" class="kg-image" alt="BM25"&gt;&lt;/figure&gt;&lt;p&gt;As you can see, this equation is really simple. Its just a modified version of TF-IDF which includes normalisation factors for the document size.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2019/11/holdup.jpg" class="kg-image" alt="BM25"&gt;&lt;/figure&gt;&lt;p&gt;Alright I am just kidding. This looks like a hungry beast trying to eat your brains out. So let's simplify things by showing the similarities with the vanilla  tf-idf and  their differences:&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2019/11/bm25-2.png" class="kg-image" alt="BM25"&gt;&lt;/figure&gt;&lt;h2 id="2-1-tf-idf"&gt;2.1 TF-IDF&lt;/h2&gt;&lt;p&gt;The ranking function of BM25 is really similar with the TF-IDF scoring function. Term frequency (TF)stands for the number of times a term appears in a document. Inverse term frequency (IDF) shows how many times a term appear across in our corpus but with a caveat. It penalises the terms that appear more frequently in the document set and increases the score of the terms that are rare. So the term frequency ( ft,d) of  a term (t) in a document (d), is the total number the term appears on that document divided by (|d|) , the total number of words in (d)&lt;/p&gt;&lt;p&gt;$$TF_{t,d} =  \frac{ f_{t,d} }{|d|}$$&lt;/p&gt;&lt;p&gt;The inverse term frequency of a term (t) shows if that particular term is common or rare across all documents. The score is calculated by dividing the total number of documents in our corpus denoted as (N), with the number of documents where this particular term was present, denoted as (n) and then taking the logarithm of that quotient&lt;/p&gt;&lt;p&gt;$$IDF_{t} = log\frac{N}{n}$$&lt;/p&gt;&lt;p&gt;The TFIDF of a single term (t) in a document (d) is their product:&lt;/p&gt;&lt;p&gt;$$TFIDF_{t,d} = \frac{ f_{t,d} }{|d|}   \ast  log\frac{N}{n}$$&lt;/p&gt;&lt;h2 id="bm25-s-tf-idf"&gt;BM25’s TF-IDF&lt;/h2&gt;&lt;p&gt;BM25 metric is very similar to TF-IDF which adds some optimisation parameters such as (k1) and (b). The IDF of BM25 is almost identical to the one from the vanilla IDF. The only difference here we subtract the number of documents a particular term appears in with the total number of documents in our corpus.This is called the probabilistic inverse document frequency.&lt;/p&gt;&lt;p&gt;$$PIDF_{t} = log\frac{N- n_{t} }{n_{t}}$$&lt;/p&gt;&lt;p&gt;The formula for TF looks more complicated, containing the optimisation parameters and a normalisation factors for the document size. The equation looks like this:&lt;/p&gt;&lt;p&gt;$$TF_{t,d} =  \frac{ f_{t,d}* ( k_{1} + 1 ) }{ f_{t,d} + k_{1} *(1-b + b *  \frac{|d|}{avg|N|} )}$$&lt;/p&gt;&lt;p&gt;When we use the vanilla TF we say the more the term appears, the more important it must be. In comparison to BM25, we set a limit. We don’t give too much advantage on documents that happen to contain a term too many times. If a document contains a term 10 times, it must be very important. If it contains the term 100 times, it is also important but not ”10 times more important”. This is exactly what (k1) is doing. This parameter sets the boundary to cap the importance of the term while it appears more frequently on the document. The differences between the vanilla TF and BM25’s TF can be seen on Figure 1. The default value of k1 in Lucene is 1.2 but we can always adjust it to increase the term importance. However it won’t affect the importance significantly since k1 is non-linear.&lt;/p&gt;&lt;p&gt;Moreover, we talked about the document size normalisation factors. In the vanilla TF, we just divide the term frequency with document size. However in a corpus we have documents with different sizes.BM25’s TF adds additional parameters to this equation to give more importance to the terms which appear in smaller documents and penalises the terms that appear in larger documents than the average document size of the corpus. This part of the equation,|d|avgdldivides the document size with the mean document size across our corpus. So if the average size is 100 and our current document’s size is 1000,then|d|avgdl= 10 whereas if our current document has a size of 10 then L = 0.1. The smaller the number in the divisor the bigger number we will get. On top of that BM25 adds a constant parameter denoted as (b) which controls to what degree document length normalises TF values. In Luce the default parameter is 0.75 and adjusting this value will increase/decrease the affect of document size on the overall TF weight.These effects are visualised on Figure 2.&lt;/p&gt;&lt;h2 id="2-3-bm25-ranking-function"&gt;2.3 BM25 ranking function&lt;/h2&gt;&lt;p&gt;BM25 is widely used in search engines to find query to document relevance. The main idea of this scoring system is to find the ”elite” words from the documents. These are words which have the highest TF-IDF score. The document score is generated by summing up all the weights for our ’elite’ words. The ranking function can be seen below. This equation will generate a score for a single document (D). (Q) stands for our query and (qi) is a single term and for each term in the query we sum up their weights.&lt;/p&gt;&lt;p&gt;$$BM25 score(D,Q) = \sum_i^n (log \frac{N-n( q_{i}) + 0.5)}{n( q_{i}) + 0.5}  )  \ast  \frac{ f(q_{i},D) \ast ( k_{1} + 1)}{f(q_{i},D) + k_{1} * (1-b+ b*  \frac{ | D | }{avgdl} ) }$$&lt;/p&gt;&lt;p&gt;In comparison to other metrics such as the Cosine distance, its scale is always from 0-100 where 100 means identical. In the BM25 ranking function we don’t have a fixed scale since the relevance score will increase depending of the query’s size or with the selected number of terms we chose to compare. In ElasticSearch this parameter is called max query terms and by default is 25&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</content:encoded></item></channel></rss>